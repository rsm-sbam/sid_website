---
title: "A Replication of Karlan and List (2007)"
author: "Siddharth Bam"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Introduction

Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the _American Economic Review_ in 2007. The article and supporting data are available from the [AEA website](https://www.aeaweb.org/articles?id=10.1257/aer.97.5.1774) and from Innovations for Poverty Action as part of [Harvard's Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/27853&version=4.2).

<!-- _to do: expand on the description of the experiment._ -->
```{r}
library(haven)
library(dplyr)
library(ggplot2)

# Load dataset from the local project directory
data <- read_dta("karlan_list_2007.dta")

# View structure of the dataset
glimpse(data)
```

This project seeks to replicate their results.


## Data

### Description

<!-- _todo: Read the data into R/Python and describe the data_ -->

```{r}
# Basic structure of the dataset
glimpse(data)

# Number of rows and columns
dim(data)

# Check missing values in key columns
colSums(is.na(data[c("gave", "amount", "treatment", "mrm2", "hpa", "years", "female")]))

# Summary statistics for key variables
summary(data[c("gave", "amount", "treatment", "mrm2", "hpa", "years", "female")])
```


:::: {.callout-note collapse="true"}
### Variable Definitions

| Variable             | Description                                                         |
|----------------------|---------------------------------------------------------------------|
| `treatment`          | Treatment                                                           |
| `control`            | Control                                                             |
| `ratio`              | Match ratio                                                         |
| `ratio2`             | 2:1 match ratio                                                     |
| `ratio3`             | 3:1 match ratio                                                     |
| `size`               | Match threshold                                                     |
| `size25`             | \$25,000 match threshold                                            |
| `size50`             | \$50,000 match threshold                                            |
| `size100`            | \$100,000 match threshold                                           |
| `sizeno`             | Unstated match threshold                                            |
| `ask`                | Suggested donation amount                                           |
| `askd1`              | Suggested donation was highest previous contribution                |
| `askd2`              | Suggested donation was 1.25 x highest previous contribution         |
| `askd3`              | Suggested donation was 1.50 x highest previous contribution         |
| `ask1`               | Highest previous contribution (for suggestion)                      |
| `ask2`               | 1.25 x highest previous contribution (for suggestion)               |
| `ask3`               | 1.50 x highest previous contribution (for suggestion)               |
| `amount`             | Dollars given                                                       |
| `gave`               | Gave anything                                                       |
| `amountchange`       | Change in amount given                                              |
| `hpa`                | Highest previous contribution                                       |
| `ltmedmra`           | Small prior donor: last gift was less than median \$35              |
| `freq`               | Number of prior donations                                           |
| `years`              | Number of years since initial donation                              |
| `year5`              | At least 5 years since initial donation                             |
| `mrm2`               | Number of months since last donation                                |
| `dormant`            | Already donated in 2005                                             |
| `female`             | Female                                                              |
| `couple`             | Couple                                                              |
| `state50one`         | State tag: 1 for one observation of each of 50 states; 0 otherwise  |
| `nonlit`             | Nonlitigation                                                       |
| `cases`              | Court cases from state in 2004-5 in which organization was involved |
| `statecnt`           | Percent of sample from state                                        |
| `stateresponse`      | Proportion of sample from the state who gave                        |
| `stateresponset`     | Proportion of treated sample from the state who gave                |
| `stateresponsec`     | Proportion of control sample from the state who gave                |
| `stateresponsetminc` | stateresponset - stateresponsec                                     |
| `perbush`            | State vote share for Bush                                           |
| `close25`            | State vote share for Bush between 47.5% and 52.5%                   |
| `red0`               | Red state                                                           |
| `blue0`              | Blue state                                                          |
| `redcty`             | Red county                                                          |
| `bluecty`            | Blue county                                                         |
| `pwhite`             | Proportion white within zip code                                    |
| `pblack`             | Proportion black within zip code                                    |
| `page18_39`          | Proportion age 18-39 within zip code                                |
| `ave_hh_sz`          | Average household size within zip code                              |
| `median_hhincome`    | Median household income within zip code                             |
| `powner`             | Proportion house owner within zip code                              |
| `psch_atlstba`       | Proportion who finished college within zip code                     |
| `pop_propurban`      | Proportion of population urban within zip code                      |

::::


### Balance Test 

As an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.

<!-- _todo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)._ -->
```{r}
# Manual t-test function
manual_t_test <- function(var, group) {
  x1 <- var[group == 1]
  x0 <- var[group == 0]

  mean_diff <- mean(x1, na.rm = TRUE) - mean(x0, na.rm = TRUE)
  se_diff <- sqrt(var(x1, na.rm = TRUE)/length(x1) + var(x0, na.rm = TRUE)/length(x0))
  t_stat <- mean_diff / se_diff
  df <- min(length(x1), length(x0)) - 1
  p_value <- 2 * pt(-abs(t_stat), df)

  data.frame(
    mean_treatment = mean(x1, na.rm = TRUE),
    mean_control = mean(x0, na.rm = TRUE),
    diff = mean_diff,
    se = se_diff,
    t = t_stat,
    df = df,
    p_value = p_value
  )
}

# Variables for balance check
vars_to_test <- c("mrm2", "hpa", "years", "female")

# Run manual t-tests
t_test_results <- lapply(vars_to_test, function(var) {
  res <- manual_t_test(data[[var]], data$treatment)
  res$variable <- var
  res
})

# Combine results
t_test_df <- do.call(rbind, t_test_results)
t_test_df
```

To test the integrity of the randomization, we compared key pre-treatment characteristics between the treatment and control groups using both manual t-tests (based on the class slide formula) and bivariate linear regressions. Specifically, we evaluated differences in:
mrm2: months since last donation
hpa: highest previous contribution
years: number of years since first donation
female: gender
In each case, the t-test and regression results matched exactly, confirming that both methods are statistically equivalent when comparing group means in randomized experiments.
None of the variables showed statistically significant differences at the 95% confidence level — all p-values were well above 0.05. This means we cannot reject the null hypothesis that these variables are balanced across groups, which reinforces the success of the random assignment process.
The only variable with a marginal p-value was female (p = 0.076), which is still not significant at the 95% level and likely reflects random variation rather than systematic bias.
This kind of analysis corresponds to Table 1 in Karlan & List (2007), which is included in the paper to reassure readers that treatment and control groups were balanced prior to intervention. This is crucial because it validates that any observed effects on donation behavior can be causally attributed to the treatment, not pre-existing differences between the groups.

## Experimental Results

### Charitable Contribution Made

First, I analyze whether matched donations lead to an increased response rate of making a donation. 

<!-- todo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control._ -->
```{r}
library(dplyr)
library(ggplot2)
library(scales)

# Recalculate donation rates
donation_rates <- data %>%
  mutate(group = ifelse(treatment == 1, "Treatment", "Control")) %>%
  group_by(group) %>%
  summarise(response_rate = mean(gave, na.rm = TRUE))

# Polished plot
ggplot(donation_rates, aes(x = group, y = response_rate, fill = group)) +
  geom_col(width = 0.6, show.legend = FALSE, color = "white") +
  geom_text(
    aes(label = percent(response_rate, accuracy = 0.1)),
    vjust = -0.6, size = 5, fontface = "bold"
  ) +
  scale_fill_manual(values = c("Control" = "#a6bddb", "Treatment" = "#3690c0")) +
  scale_y_continuous(
    limits = c(0, 0.03),
    labels = percent_format(accuracy = 0.1),
    expand = expansion(mult = c(0, 0.05))
  ) +
  labs(
    title = "Proportion of People Who Donated",
    x = NULL,
    y = "Donation Rate"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text = element_text(size = 12),
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5)
  )
```


<!-- _todo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)_ -->
```{r}
# T-test on donation rate (binary outcome: gave)
t_test_gave <- t.test(gave ~ treatment, data = data)
t_test_gave
```
```{r}
# Bivariate linear regression: Pr(Give) ~ Treatment
reg_gave <- lm(gave ~ treatment, data = data)
summary(reg_gave)
```

We conducted both a t-test and a bivariate linear regression to evaluate whether assignment to the treatment group (which received a matching or challenge grant letter) led to a higher likelihood of donating, compared to the control group.
The t-test results show that the difference in donation rates between the groups is statistically significant at the 1% level, with treatment recipients donating at a higher rate. Specifically, the treatment group had an average donation rate of ~2.2%, compared to ~1.8% for the control group. The regression confirms this, estimating a treatment effect of roughly 0.4 percentage points — the exact same difference found in the t-test.
While the effect size is relatively small in absolute terms, the statistical significance suggests that this difference is unlikely to be due to random chance. This finding aligns with Panel A of Table 2a in Karlan & List (2007), which similarly reports a statistically significant increase in response due to treatment.
From a behavioral perspective, this result highlights that how a charitable appeal is framed can influence people's decisions. Even though the economic “ask” is the same, donors appear more likely to give when the appeal includes a matching or challenge element. This could be due to a perception that their donation has more impact, or a psychological trigger related to urgency, social proof, or shared responsibility.
In short, presentation matters, and even small nudges in message framing can lead to measurable changes in behavior, especially in the context of fundraising and prosocial giving.

<!-- _todo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper._ -->
```{r}
# Load package for probit model
library(stats)

# Run probit regression: Pr(Give) ~ Treatment
probit_model <- glm(gave ~ treatment, data = data, family = binomial(link = "probit"))

# Display summary
summary(probit_model)
```

We estimated a probit regression to model the probability of donation (gave) as a function of treatment assignment. The estimated coefficient on treatment is 0.087, which is statistically significant at the 1% level (p ≈ 0.0019). This result confirms the pattern shown in Table 3, Column 1 of Karlan & List (2007), where the authors also report a positive and significant treatment effect.
The positive sign of the treatment coefficient indicates that receiving a matching or challenge grant letter increases the likelihood of donating, consistent with the findings from the linear regression and t-tests. Although the magnitude of the probit coefficient itself is not directly interpretable in terms of probabilities, its significance and direction confirm that the treatment has a positive impact on the outcome.
This result adds further robustness to the conclusion that framing a charitable appeal using matching mechanisms influences behavior, reinforcing the causal interpretation of the treatment effect.

### Differences between Match Rates

Next, I assess the effectiveness of different sizes of matched donations on the response rate.

<!-- _todo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the "figures suggest" comment the authors make on page 8?_ -->
```{r}
# Filter for treatment group with valid match ratios only
match_data <- data %>%
  filter(treatment == 1 & (ratio == 1 | ratio2 == 1 | ratio3 == 1)) %>%
  mutate(
    match_type = case_when(
      ratio == 1 ~ "1:1",
      ratio2 == 1 ~ "2:1",
      ratio3 == 1 ~ "3:1"
    )
  )
```
```{r}
# 2:1 match vs 1:1 match
t_test_2v1 <- t.test(gave ~ match_type, data = filter(match_data, match_type %in% c("1:1", "2:1")))
print("T-test: 2:1 vs 1:1")
print(t_test_2v1)

# 3:1 match vs 1:1 match
t_test_3v1 <- t.test(gave ~ match_type, data = filter(match_data, match_type %in% c("1:1", "3:1")))
print("T-test: 3:1 vs 1:1")
print(t_test_3v1)
```

We ran t-tests to compare the likelihood of donating under different match ratios within the treatment group: 1:1, 2:1, and 3:1. The results show no statistically significant differences in donation rates between the 1:1 group and either the 2:1 or 3:1 groups. In both comparisons, p-values were well above the 0.05 threshold, and the confidence intervals include zero.
These findings support the authors' statement on page 8 that “the figures suggest that the match offer alone, rather than the size of the match, is the primary driver of increased giving.” In other words, once a match is offered, increasing the match ratio does not lead to significantly higher response rates.
This suggests that the behavioral mechanism behind the treatment effect is not a financial calculation of impact, but rather the presence of a match signal itself — which may boost perceived legitimacy, urgency, or moral obligation, regardless of the multiplier.

<!-- _todo: Assess the same issue using a regression. Specifically, create the variable `ratio1` then regress `gave` on `ratio1`, `ratio2`, and `ratio3` (or alternatively, regress `gave` on the categorical variable `ratio`). Interpret the coefficients and their statistical precision._ -->
```{r}
# Create binary indicators
match_data <- match_data %>%
  mutate(
    ratio1 = ifelse(ratio == 1, 1, 0),
    ratio2 = ifelse(ratio2 == 1, 1, 0),
    ratio3 = ifelse(ratio3 == 1, 1, 0)
  )

# Regression with dummy variables
model_dummies <- lm(gave ~ ratio1 + ratio2 + ratio3, data = match_data)
summary(model_dummies)
```

To assess whether the size of the match ratio affects the likelihood of donating, we created dummy variables for the 1:1, 2:1, and 3:1 match conditions and regressed gave on these indicators. The results show no statistically significant differences in donation likelihood between match sizes.
The regression omits the ratio3 variable due to perfect multicollinearity (this happens because all three dummy variables together fully capture the variation in match type — i.e., there’s no baseline). In this setup, the intercept represents the average donation rate for the 3:1 group, and the coefficients on ratio1 and ratio2 reflect the difference in donation likelihood between the 1:1 or 2:1 groups and the 3:1 group.
Both coefficients are small and not statistically significant (p > 0.3 and p > 0.95), confirming that increasing the match ratio does not lead to a higher probability of donation.
This mirrors our earlier t-test findings and reinforces the authors' point: the existence of a match, not its magnitude, is what influences behavior. Donors appear to be responding to the presence of a match as a signal or motivator, rather than calculating a greater expected impact based on the match size.

<!-- _todo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios.  Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?_ -->
```{r}
# Direct response rate comparison
match_rates <- match_data %>%
  group_by(match_type) %>%
  summarise(response_rate = mean(gave, na.rm = TRUE))

# Extract rates
rate_1to1 <- match_rates$response_rate[match_rates$match_type == "1:1"]
rate_2to1 <- match_rates$response_rate[match_rates$match_type == "2:1"]
rate_3to1 <- match_rates$response_rate[match_rates$match_type == "3:1"]

# Differences
diff_2v1 <- rate_2to1 - rate_1to1
diff_3v2 <- rate_3to1 - rate_2to1

# Print
cat("Difference (2:1 - 1:1):", diff_2v1, "\n")
cat("Difference (3:1 - 2:1):", diff_3v2, "\n")
```
```{r}
# Extract regression coefficients
coefs <- coef(model_dummies)

# Differences
coef_2v1 <- coefs["ratio2"] - coefs["ratio1"]
coef_3v2 <- coefs["ratio3"] - coefs["ratio2"]

# Print
cat("Coefficient Difference (2:1 - 1:1):", coef_2v1, "\n")
cat("Coefficient Difference (3:1 - 2:1):", coef_3v2, "\n")
```

We compared donation response rates across different match ratios both directly from the data and using the regression coefficients from our earlier model.
From the raw data, the response rate for the 2:1 match was 0.00188 higher than the 1:1 match, and the difference between 3:1 and 2:1 was only 0.00010 — both extremely small.
These exact differences were also reflected in the fitted coefficients of the regression model (with the exception of the 3:1 coefficient, which was excluded due to multicollinearity). The match between the empirical and model-based differences reinforces the reliability of our findings.
These results confirm that increasing the match ratio does not substantially increase the likelihood of donation. This supports the paper’s central insight: it’s the presence of a match — not its size — that matters most. Donors are likely influenced by the match offer as a behavioral nudge or psychological signal, rather than through careful evaluation of the multiplier.

### Size of Charitable Contribution

In this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.

<!-- _todo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?_ -->
```{r}
# T-test on amount donated (includes zero for non-donors)
t_test_amount <- t.test(amount ~ treatment, data = data)
t_test_amount
```
```{r}
# Linear regression: donation amount ~ treatment
reg_amount <- lm(amount ~ treatment, data = data)
summary(reg_amount)
```

We analyzed whether being assigned to a treatment group (receiving a matching or challenge grant letter) influenced the amount donated, including non-donors (i.e., those who gave $0).
The t-test shows a higher mean donation amount in the treatment group ($0.97) compared to the control group ($0.81), but this difference is not statistically significant at the 5% level (p ≈ 0.055). Similarly, the linear regression estimates an increase of $0.15 in average donation for the treatment group, with a p-value of ~0.063 — again, just above the typical threshold for statistical significance.
These results suggest that the treatment may have a small positive effect on donation size, but the evidence is not strong enough to draw a firm conclusion. The primary effect of the treatment seems to be on whether a person donates at all, not on how much they donate once they decide to give.
This finding reinforces the interpretation that message framing (e.g., offering a match) is more effective at triggering action than at increasing generosity among existing donors. Behavioral nudges can shift decisions, but their effect on donation magnitude appears more limited in this context.

<!-- _todo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients -- what did we learn? Does the treatment coefficient have a causal interpretation?_  -->
```{r}
# Filter only those who donated
donors <- data %>% filter(gave == 1)

# T-test of amount among donors
t_test_donors <- t.test(amount ~ treatment, data = donors)
t_test_donors
```
```{r}
# Linear regression: amount ~ treatment among donors only
reg_donors <- lm(amount ~ treatment, data = donors)
summary(reg_donors)
```

We repeated our analysis of donation amount, this time limiting the sample to only those individuals who made a donation. This allows us to assess whether the treatment influenced how much people gave, conditional on choosing to give.
Both the t-test and the linear regression show no statistically significant difference in average donation size between treatment and control groups among donors. The regression estimates that donors in the treatment group gave about $1.67 less, on average, than those in the control group — but this difference is small, statistically insignificant (p = 0.56), and could easily be explained by chance.
Importantly, the treatment coefficient in this regression does not have a causal interpretation. Once we restrict the sample to donors only, we lose the balance created by random assignment. That is, the decision to donate is influenced by treatment, so analyzing only those who gave introduces selection bias. The treatment and control groups in this subset may differ in unobserved ways that affect donation size.
Overall, this analysis supports the conclusion that the treatment increases the likelihood of giving, but does not significantly change the donation amount among those who do give. The main behavioral effect appears to be in motivating action, not increasing generosity.

<!-- todo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot._ -->
```{r}
library(ggplot2)
library(dplyr)

# Filter for donors only and add group labels
donors <- data %>%
  filter(gave == 1) %>%
  mutate(group = ifelse(treatment == 1, "Treatment", "Control"))

# Calculate mean donation for each group
group_means <- donors %>%
  group_by(group) %>%
  summarise(mean_amount = mean(amount, na.rm = TRUE))

# Join means for plotting
donors <- donors %>%
  left_join(group_means, by = "group")

# Faceted histogram plot
ggplot(donors, aes(x = amount)) +
  geom_histogram(binwidth = 5, fill = "#3182bd", color = "white", alpha = 0.9) +
  geom_vline(aes(xintercept = mean_amount), color = "#e34a33", linetype = "dashed", linewidth = 1.2) +
  facet_wrap(~ group, ncol = 2, scales = "free_y") +
  labs(
    title = "Distribution of Donation Amounts Among Donors",
    x = "Donation Amount ($)",
    y = "Number of Donors"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    strip.text = element_text(size = 14, face = "bold"),
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.title = element_text(size = 13),
    axis.text = element_text(size = 12),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  )
```



## Simulation Experiment

As a reminder of how the t-statistic "works," in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.

Suppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. 

Further suppose that the true distribution of respondents who do get a charitable donation match of any size  is Bernoulli with probability p=0.022 that a donation is made.

### Law of Large Numbers

<!-- _to do:  Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You'll then calculate a vector of 10,000 differences, and then you'll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means._ -->
```{r}
set.seed(123)  # for reproducibility

# Simulate binary outcomes
control_draws <- rbinom(100000, 1, 0.018)
treatment_draws <- rbinom(100000, 1, 0.022)

# Compute vector of differences
differences <- treatment_draws[1:10000] - control_draws[1:10000]

# Compute cumulative average
cumulative_avg <- cumsum(differences) / seq_along(differences)

# Plot cumulative average
plot(cumulative_avg, type = "l", col = "steelblue", lwd = 2,
     xlab = "Number of Observations",
     ylab = "Cumulative Average of Differences",
     main = "Law of Large Numbers: Cumulative Difference in Response Rates")
abline(h = 0.022 - 0.018, col = "red", lty = "dashed", lwd = 2)  # true difference
legend("topright", legend = c("Cumulative Average", "True Difference (0.004)"),
       col = c("steelblue", "red"), lty = c(1, 2), bty = "n")
```

The plot above demonstrates the Law of Large Numbers using simulated donation behavior. We generated 10,000 paired binary draws from the control and treatment distributions (with donation probabilities of 1.8% and 2.2%, respectively). For each pair, we calculated the difference in outcomes and then plotted the cumulative average of those differences.
Early in the sequence, the cumulative average fluctuates due to random variation in small samples. However, as the number of observations increases, the cumulative average begins to stabilize and converge toward the true difference in population means — 0.004, shown by the red dashed line.
This illustrates the Law of Large Numbers in action: as the sample size grows, sample averages tend to approach the population average. In the context of this experiment, it shows how small effects — like a 0.4 percentage point increase in donation likelihood — become visible and reliable only when we observe enough data.

### Central Limit Theorem

<!-- _to do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the "middle" of the distribution or whether it's in the "tail."_ -->

```{r}
set.seed(42)

# True probabilities
p_control <- 0.018
p_treatment <- 0.022

# Sample sizes to simulate
sample_sizes <- c(50, 200, 500, 1000)

# Function to simulate sample mean differences
simulate_diff_means <- function(n, reps = 1000) {
  replicate(reps, {
    treat_sample <- rbinom(n, 1, p_treatment)
    control_sample <- rbinom(n, 1, p_control)
    mean(treat_sample) - mean(control_sample)
  })
}

# Simulate distributions for each sample size
diffs_list <- lapply(sample_sizes, simulate_diff_means)

# Load for multi-panel plotting
par(mfrow = c(2, 2))  # 2 rows, 2 columns
for (i in seq_along(diffs_list)) {
  hist(diffs_list[[i]],
       main = paste("Sample Size:", sample_sizes[i]),
       xlab = "Mean Difference",
       col = "lightblue",
       breaks = 30,
       border = "white")
  abline(v = 0, col = "red", lwd = 2, lty = "dashed")  # mark zero
}
```

These four histograms visualize the Central Limit Theorem by showing the distribution of average differences in donation rates between treatment and control groups, simulated at different sample sizes: 50, 200, 500, and 1000.
For each sample size, we ran 1000 simulations. In each iteration, we drew random samples from both treatment and control distributions (with donation probabilities of 2.2% and 1.8%, respectively), calculated the average difference, and plotted the results.
As the sample size increases:
The distribution becomes more bell-shaped and symmetric.
The spread decreases, showing less variability in the average difference.
The center of the distribution shifts away from zero, reflecting the true difference of 0.004.
This demonstrates the Central Limit Theorem in action: as sample size increases, the sampling distribution of the mean difference becomes approximately normal, regardless of the original binary distribution. Also, zero is clearly in the tail (especially for n = 500 and n = 1000), providing evidence that the treatment effect is not due to chance.
These plots reinforce the experimental finding — that even a small difference in probabilities becomes statistically distinguishable with enough data.



