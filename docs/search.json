[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "This is Project 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nSiddharth Bam\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nSiddharth Bam\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Cars\n\n\n\n\n\n\nSiddharth Bam\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project4/hw2_questions.html",
    "href": "blog/project4/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n# Load necessary library\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Read the Blueprinty dataset\nblueprinty &lt;- read_csv(\"/Users/siddharthbam/Desktop/marketing Analytics/sid_site/blog/project4/blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# View the first few rows of the dataset\nhead(blueprinty)\n\n# A tibble: 6 × 4\n  patents region      age iscustomer\n    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1       0 Midwest    32.5          0\n2       3 Southwest  37.5          0\n3       4 Northwest  27            1\n4       3 Northeast  24.5          0\n5       3 Southwest  37            0\n6       6 Northeast  29.5          1\n\n\n\n# Check column names\ncolnames(blueprinty)\n\n[1] \"patents\"    \"region\"     \"age\"        \"iscustomer\"\n\n# Also look at the first few rows to see the structure\nhead(blueprinty)\n\n# A tibble: 6 × 4\n  patents region      age iscustomer\n    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1       0 Midwest    32.5          0\n2       3 Southwest  37.5          0\n3       4 Northwest  27            1\n4       3 Northeast  24.5          0\n5       3 Southwest  37            0\n6       6 Northeast  29.5          1\n\n\n\n\n# Convert iscustomer to a labeled factor\nblueprinty &lt;- blueprinty %&gt;%\n  mutate(customer = factor(iscustomer, labels = c(\"Non-Customer\", \"Customer\")))\n\n# Plot histogram of number of patents by customer status\nggplot(blueprinty, aes(x = patents, fill = customer)) +\n  geom_histogram(binwidth = 1, position = \"dodge\") +\n  labs(title = \"Histogram of Patents by Customer Status\", x = \"Number of Patents\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\nlibrary(knitr)\n# Calculate and display mean number of patents by customer status as a table\nblueprinty %&gt;%\n  group_by(customer) %&gt;%\n  summarize(mean_patents = mean(patents), .groups = \"drop\") %&gt;%\n  kable(digits = 2, caption = \"Mean Number of Patents by Customer Status\")\n\n\nMean Number of Patents by Customer Status\n\n\ncustomer\nmean_patents\n\n\n\n\nNon-Customer\n3.47\n\n\nCustomer\n4.13\n\n\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n# Compare region distribution by customer status\nggplot(blueprinty, aes(x = region, fill = customer)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Region Distribution by Customer Status\", x = \"Region\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Compare age distribution by customer status\nggplot(blueprinty, aes(x = age, fill = customer)) +\n  geom_histogram(binwidth = 5, position = \"dodge\") +\n  labs(title = \"Histogram of Firm Age by Customer Status\", x = \"Firm Age (Years)\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\nlibrary(knitr)\n\n# Compare mean age by customer status and display as a table\nblueprinty %&gt;%\n  group_by(customer) %&gt;%\n  summarize(mean_age = mean(age), .groups = \"drop\") %&gt;%\n  kable(digits = 1, caption = \"Mean Firm Age by Customer Status\")\n\n\nMean Firm Age by Customer Status\n\n\ncustomer\nmean_age\n\n\n\n\nNon-Customer\n26.1\n\n\nCustomer\n26.9\n\n\n\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nLet \\(Y_1, Y_2, \\dots, Y_n\\) be independent observations such that \\(Y_i \\sim \\text{Poisson}(\\lambda)\\).\nThen the likelihood function is: \\[\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\] Taking the natural logarithm, the log-likelihood becomes:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n\\]\n\n\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(-Inf)\n  sum(Y * log(lambda) - lambda - lgamma(Y + 1))\n}\n\n\n\n# Define the Poisson log-likelihood function\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) {\n    return(-Inf)  # log-likelihood is undefined for non-positive lambda\n  }\n  sum(Y * log(lambda) - lambda - lgamma(Y + 1))\n}\n\n\n\n# Create a sequence of lambda values to evaluate\nlambda_vals &lt;- seq(0.1, 10, by = 0.1)\n\n# Compute log-likelihood for each lambda\nloglik_vals &lt;- sapply(lambda_vals, poisson_loglikelihood, Y = blueprinty$patents)\n\n# Plot lambda vs log-likelihood\nplot(lambda_vals, loglik_vals, type = \"l\",\n     main = \"Log-Likelihood of Poisson Model\",\n     xlab = expression(lambda), ylab = \"Log-Likelihood\",\n     col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nLet \\(Y_1, Y_2, \\dots, Y_n \\overset{iid}{\\sim} \\text{Poisson}(\\lambda)\\), and recall that the log-likelihood function is:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n= -n\\lambda + \\left(\\sum_{i=1}^n Y_i\\right) \\log \\lambda + \\text{const}\n\\]\nTo find the MLE, we take the derivative with respect to \\(\\lambda\\) and set it equal to zero:\n\\[\n\\frac{d\\ell}{d\\lambda} = -n + \\frac{\\sum Y_i}{\\lambda} = 0\n\\]\nSolving for \\(\\lambda\\) gives:\n\\[\n\\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThis result is intuitive because, in a Poisson distribution, the mean is \\(\\lambda\\), making the sample mean \\(\\bar{Y}\\) a natural and logical estimator for \\(\\lambda\\).\n\n\n# Use optim to maximize the Poisson log-likelihood function\nmle_result &lt;- optim(\n  par = 1,  # initial guess for lambda\n  fn = function(lambda) -poisson_loglikelihood(lambda, blueprinty$patents),  # we minimize negative log-likelihood\n  method = \"Brent\",  # since lambda is one-dimensional\n  lower = 0.01,      # lambda must be positive\n  upper = 20         # reasonable upper bound\n)\n\n# Print the MLE result\nmle_result$par  # This is the estimated lambda (MLE)\n\n[1] 3.684667\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n# Define the log-likelihood for Poisson regression\npoisson_regression_likelihood &lt;- function(beta, Y, X) {\n  eta &lt;- X %*% beta                  # linear predictor: Xβ\n  lambda &lt;- exp(eta)                # inverse link: λ = exp(Xβ)\n  \n  loglik &lt;- sum(Y * log(lambda) - lambda - lgamma(Y + 1))\n  return(loglik)\n}\n\n\n\n# Ensure necessary library is loaded\nlibrary(tidyverse)\n\n# Prepare covariates\nblueprinty &lt;- blueprinty %&gt;%\n  mutate(\n    age_sq = age^2,\n    region = factor(region),\n    iscustomer = as.numeric(iscustomer),\n    customer = as.numeric(iscustomer) # preserve naming from earlier\n  )\n\n# Create design matrix X\nX &lt;- model.matrix(~ age + age_sq + region + customer, data = blueprinty)\n\n# Response variable\nY &lt;- blueprinty$patents\n\n# Define the Poisson regression log-likelihood (to maximize)\npoisson_regression_likelihood &lt;- function(beta, Y, X) {\n  eta &lt;- X %*% beta\n  lambda &lt;- exp(eta)\n  sum(Y * log(lambda) - lambda - lgamma(Y + 1))\n}\n\n# Negative log-likelihood for minimization\nneg_loglik &lt;- function(beta) {\n  -poisson_regression_likelihood(beta, Y, X)\n}\n\n# Initial values for beta\ninit_beta &lt;- rep(0, ncol(X))\n\n# Optimization\nfit &lt;- optim(\n  par = init_beta,\n  fn = neg_loglik,\n  hessian = TRUE,\n  method = \"BFGS\"\n)\n\n# Extract estimates and compute standard errors\nbeta_hat &lt;- fit$par\nhessian &lt;- fit$hessian\nvar_beta &lt;- solve(hessian)         # Inverse Hessian = variance-covariance matrix\nse_beta &lt;- sqrt(diag(var_beta))    # Standard errors\n\n# Create summary table\nresults &lt;- tibble(\n  Term = colnames(X),\n  Estimate = beta_hat,\n  Std_Error = se_beta\n)\n\nresults\n\n# A tibble: 8 × 3\n  Term            Estimate Std_Error\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     -0.126   0.112    \n2 age              0.116   0.00636  \n3 age_sq          -0.00223 0.0000771\n4 regionNortheast -0.0246  0.0434   \n5 regionNorthwest -0.0348  0.0529   \n6 regionSouth     -0.00544 0.0524   \n7 regionSouthwest -0.0378  0.0472   \n8 customer         0.0607  0.0321   \n\n\n\n\n# Fit Poisson regression model using glm\nglm_fit &lt;- glm(patents ~ age + I(age^2) + region + customer, \n               data = blueprinty, \n               family = poisson(link = \"log\"))\n\n# Summary of the model\nsummary(glm_fit)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + customer, family = poisson(link = \"log\"), \n    data = blueprinty)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \ncustomer         0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nThe Poisson regression shows that Blueprinty customers file more patents than non-customers, even after controlling for firm age and region. The customer coefficient is positive and statistically significant, indicating that using Blueprinty’s software is associated with a higher expected number of patents. Age has a nonlinear effect, suggesting patenting increases with firm age up to a point and then declines. Regional differences also affect patent counts relative to the baseline region.\n\n\n# Use fitted coefficients from glm\nbeta_hat &lt;- coef(glm_fit)\n\n# Create X_0: all firms set as non-customers\nblueprinty_X0 &lt;- blueprinty %&gt;%\n  mutate(customer = 0)\nX_0 &lt;- model.matrix(~ age + I(age^2) + region + customer, data = blueprinty_X0)\n\n# Create X_1: all firms set as customers\nblueprinty_X1 &lt;- blueprinty %&gt;%\n  mutate(customer = 1)\nX_1 &lt;- model.matrix(~ age + I(age^2) + region + customer, data = blueprinty_X1)\n\n# Predict patent counts under each scenario\ny_pred_0 &lt;- exp(X_0 %*% beta_hat)\ny_pred_1 &lt;- exp(X_1 %*% beta_hat)\n\n# Compute average difference\ntreatment_effect &lt;- mean(y_pred_1 - y_pred_0)\ntreatment_effect\n\n[1] 0.7927681\n\n\nThe average treatment effect estimated from the Poisson regression model is approximately 0.79. This means that, holding age and region constant, firms that use Blueprinty’s software are predicted to receive about 0.79 more patents over 5 years compared to firms that do not use the software."
  },
  {
    "objectID": "blog/project4/hw2_questions.html#blueprinty-case-study",
    "href": "blog/project4/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n# Load necessary library\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Read the Blueprinty dataset\nblueprinty &lt;- read_csv(\"/Users/siddharthbam/Desktop/marketing Analytics/sid_site/blog/project4/blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# View the first few rows of the dataset\nhead(blueprinty)\n\n# A tibble: 6 × 4\n  patents region      age iscustomer\n    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1       0 Midwest    32.5          0\n2       3 Southwest  37.5          0\n3       4 Northwest  27            1\n4       3 Northeast  24.5          0\n5       3 Southwest  37            0\n6       6 Northeast  29.5          1\n\n\n\n# Check column names\ncolnames(blueprinty)\n\n[1] \"patents\"    \"region\"     \"age\"        \"iscustomer\"\n\n# Also look at the first few rows to see the structure\nhead(blueprinty)\n\n# A tibble: 6 × 4\n  patents region      age iscustomer\n    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1       0 Midwest    32.5          0\n2       3 Southwest  37.5          0\n3       4 Northwest  27            1\n4       3 Northeast  24.5          0\n5       3 Southwest  37            0\n6       6 Northeast  29.5          1\n\n\n\n\n# Convert iscustomer to a labeled factor\nblueprinty &lt;- blueprinty %&gt;%\n  mutate(customer = factor(iscustomer, labels = c(\"Non-Customer\", \"Customer\")))\n\n# Plot histogram of number of patents by customer status\nggplot(blueprinty, aes(x = patents, fill = customer)) +\n  geom_histogram(binwidth = 1, position = \"dodge\") +\n  labs(title = \"Histogram of Patents by Customer Status\", x = \"Number of Patents\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\nlibrary(knitr)\n# Calculate and display mean number of patents by customer status as a table\nblueprinty %&gt;%\n  group_by(customer) %&gt;%\n  summarize(mean_patents = mean(patents), .groups = \"drop\") %&gt;%\n  kable(digits = 2, caption = \"Mean Number of Patents by Customer Status\")\n\n\nMean Number of Patents by Customer Status\n\n\ncustomer\nmean_patents\n\n\n\n\nNon-Customer\n3.47\n\n\nCustomer\n4.13\n\n\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n# Compare region distribution by customer status\nggplot(blueprinty, aes(x = region, fill = customer)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Region Distribution by Customer Status\", x = \"Region\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Compare age distribution by customer status\nggplot(blueprinty, aes(x = age, fill = customer)) +\n  geom_histogram(binwidth = 5, position = \"dodge\") +\n  labs(title = \"Histogram of Firm Age by Customer Status\", x = \"Firm Age (Years)\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\nlibrary(knitr)\n\n# Compare mean age by customer status and display as a table\nblueprinty %&gt;%\n  group_by(customer) %&gt;%\n  summarize(mean_age = mean(age), .groups = \"drop\") %&gt;%\n  kable(digits = 1, caption = \"Mean Firm Age by Customer Status\")\n\n\nMean Firm Age by Customer Status\n\n\ncustomer\nmean_age\n\n\n\n\nNon-Customer\n26.1\n\n\nCustomer\n26.9\n\n\n\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nLet \\(Y_1, Y_2, \\dots, Y_n\\) be independent observations such that \\(Y_i \\sim \\text{Poisson}(\\lambda)\\).\nThen the likelihood function is: \\[\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\] Taking the natural logarithm, the log-likelihood becomes:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n\\]\n\n\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(-Inf)\n  sum(Y * log(lambda) - lambda - lgamma(Y + 1))\n}\n\n\n\n# Define the Poisson log-likelihood function\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) {\n    return(-Inf)  # log-likelihood is undefined for non-positive lambda\n  }\n  sum(Y * log(lambda) - lambda - lgamma(Y + 1))\n}\n\n\n\n# Create a sequence of lambda values to evaluate\nlambda_vals &lt;- seq(0.1, 10, by = 0.1)\n\n# Compute log-likelihood for each lambda\nloglik_vals &lt;- sapply(lambda_vals, poisson_loglikelihood, Y = blueprinty$patents)\n\n# Plot lambda vs log-likelihood\nplot(lambda_vals, loglik_vals, type = \"l\",\n     main = \"Log-Likelihood of Poisson Model\",\n     xlab = expression(lambda), ylab = \"Log-Likelihood\",\n     col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nLet \\(Y_1, Y_2, \\dots, Y_n \\overset{iid}{\\sim} \\text{Poisson}(\\lambda)\\), and recall that the log-likelihood function is:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n= -n\\lambda + \\left(\\sum_{i=1}^n Y_i\\right) \\log \\lambda + \\text{const}\n\\]\nTo find the MLE, we take the derivative with respect to \\(\\lambda\\) and set it equal to zero:\n\\[\n\\frac{d\\ell}{d\\lambda} = -n + \\frac{\\sum Y_i}{\\lambda} = 0\n\\]\nSolving for \\(\\lambda\\) gives:\n\\[\n\\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThis result is intuitive because, in a Poisson distribution, the mean is \\(\\lambda\\), making the sample mean \\(\\bar{Y}\\) a natural and logical estimator for \\(\\lambda\\).\n\n\n# Use optim to maximize the Poisson log-likelihood function\nmle_result &lt;- optim(\n  par = 1,  # initial guess for lambda\n  fn = function(lambda) -poisson_loglikelihood(lambda, blueprinty$patents),  # we minimize negative log-likelihood\n  method = \"Brent\",  # since lambda is one-dimensional\n  lower = 0.01,      # lambda must be positive\n  upper = 20         # reasonable upper bound\n)\n\n# Print the MLE result\nmle_result$par  # This is the estimated lambda (MLE)\n\n[1] 3.684667\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n# Define the log-likelihood for Poisson regression\npoisson_regression_likelihood &lt;- function(beta, Y, X) {\n  eta &lt;- X %*% beta                  # linear predictor: Xβ\n  lambda &lt;- exp(eta)                # inverse link: λ = exp(Xβ)\n  \n  loglik &lt;- sum(Y * log(lambda) - lambda - lgamma(Y + 1))\n  return(loglik)\n}\n\n\n\n# Ensure necessary library is loaded\nlibrary(tidyverse)\n\n# Prepare covariates\nblueprinty &lt;- blueprinty %&gt;%\n  mutate(\n    age_sq = age^2,\n    region = factor(region),\n    iscustomer = as.numeric(iscustomer),\n    customer = as.numeric(iscustomer) # preserve naming from earlier\n  )\n\n# Create design matrix X\nX &lt;- model.matrix(~ age + age_sq + region + customer, data = blueprinty)\n\n# Response variable\nY &lt;- blueprinty$patents\n\n# Define the Poisson regression log-likelihood (to maximize)\npoisson_regression_likelihood &lt;- function(beta, Y, X) {\n  eta &lt;- X %*% beta\n  lambda &lt;- exp(eta)\n  sum(Y * log(lambda) - lambda - lgamma(Y + 1))\n}\n\n# Negative log-likelihood for minimization\nneg_loglik &lt;- function(beta) {\n  -poisson_regression_likelihood(beta, Y, X)\n}\n\n# Initial values for beta\ninit_beta &lt;- rep(0, ncol(X))\n\n# Optimization\nfit &lt;- optim(\n  par = init_beta,\n  fn = neg_loglik,\n  hessian = TRUE,\n  method = \"BFGS\"\n)\n\n# Extract estimates and compute standard errors\nbeta_hat &lt;- fit$par\nhessian &lt;- fit$hessian\nvar_beta &lt;- solve(hessian)         # Inverse Hessian = variance-covariance matrix\nse_beta &lt;- sqrt(diag(var_beta))    # Standard errors\n\n# Create summary table\nresults &lt;- tibble(\n  Term = colnames(X),\n  Estimate = beta_hat,\n  Std_Error = se_beta\n)\n\nresults\n\n# A tibble: 8 × 3\n  Term            Estimate Std_Error\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     -0.126   0.112    \n2 age              0.116   0.00636  \n3 age_sq          -0.00223 0.0000771\n4 regionNortheast -0.0246  0.0434   \n5 regionNorthwest -0.0348  0.0529   \n6 regionSouth     -0.00544 0.0524   \n7 regionSouthwest -0.0378  0.0472   \n8 customer         0.0607  0.0321   \n\n\n\n\n# Fit Poisson regression model using glm\nglm_fit &lt;- glm(patents ~ age + I(age^2) + region + customer, \n               data = blueprinty, \n               family = poisson(link = \"log\"))\n\n# Summary of the model\nsummary(glm_fit)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + customer, family = poisson(link = \"log\"), \n    data = blueprinty)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \ncustomer         0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nThe Poisson regression shows that Blueprinty customers file more patents than non-customers, even after controlling for firm age and region. The customer coefficient is positive and statistically significant, indicating that using Blueprinty’s software is associated with a higher expected number of patents. Age has a nonlinear effect, suggesting patenting increases with firm age up to a point and then declines. Regional differences also affect patent counts relative to the baseline region.\n\n\n# Use fitted coefficients from glm\nbeta_hat &lt;- coef(glm_fit)\n\n# Create X_0: all firms set as non-customers\nblueprinty_X0 &lt;- blueprinty %&gt;%\n  mutate(customer = 0)\nX_0 &lt;- model.matrix(~ age + I(age^2) + region + customer, data = blueprinty_X0)\n\n# Create X_1: all firms set as customers\nblueprinty_X1 &lt;- blueprinty %&gt;%\n  mutate(customer = 1)\nX_1 &lt;- model.matrix(~ age + I(age^2) + region + customer, data = blueprinty_X1)\n\n# Predict patent counts under each scenario\ny_pred_0 &lt;- exp(X_0 %*% beta_hat)\ny_pred_1 &lt;- exp(X_1 %*% beta_hat)\n\n# Compute average difference\ntreatment_effect &lt;- mean(y_pred_1 - y_pred_0)\ntreatment_effect\n\n[1] 0.7927681\n\n\nThe average treatment effect estimated from the Poisson regression model is approximately 0.79. This means that, holding age and region constant, firms that use Blueprinty’s software are predicted to receive about 0.79 more patents over 5 years compared to firms that do not use the software."
  },
  {
    "objectID": "blog/project4/hw2_questions.html#airbnb-case-study",
    "href": "blog/project4/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\n# Load required libraries\nlibrary(tidyverse)\n\n# Read in the Airbnb data\nairbnb &lt;- read_csv(\"airbnb.csv\")\n\nNew names:\nRows: 40628 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): last_scraped, host_since, room_type dbl (10): ...1, id, days, bathrooms,\nbedrooms, price, number_of_reviews, rev... lgl (1): instant_bookable\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n# Clean and preprocess data\nairbnb_clean &lt;- airbnb %&gt;%\n  select(number_of_reviews, room_type, bathrooms, bedrooms, price,\n         review_scores_cleanliness, review_scores_location,\n         review_scores_value, instant_bookable) %&gt;%\n  drop_na() %&gt;%\n  mutate(\n    room_type = factor(room_type),\n    instant_bookable = factor(instant_bookable)\n  )\n\n\n# Histogram of Number of Reviews\nggplot(airbnb_clean, aes(x = number_of_reviews)) +\n  geom_histogram(binwidth = 5, fill = \"#69b3a2\", color = \"white\") +\n  labs(\n    title = \"Distribution of Number of Reviews\",\n    x = \"Number of Reviews\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Boxplot: Reviews by Room Type\nggplot(airbnb_clean, aes(x = room_type, y = number_of_reviews)) +\n  geom_boxplot(fill = \"#fbb4ae\") +\n  labs(\n    title = \"Number of Reviews by Room Type\",\n    x = \"Room Type\",\n    y = \"Number of Reviews\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Scatter Plot: Price vs. Number of Reviews (log scale for price)\nggplot(airbnb_clean, aes(x = price, y = number_of_reviews)) +\n  geom_point(alpha = 0.4) +\n  scale_x_log10() +\n  labs(\n    title = \"Number of Reviews vs. Price (Log Scale)\",\n    x = \"Price (log scale)\",\n    y = \"Number of Reviews\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Bar Plot: Count of Instant Bookable Listings\nggplot(airbnb_clean, aes(x = instant_bookable)) +\n  geom_bar(fill = \"#80b1d3\") +\n  labs(\n    title = \"Count of Instant Bookable Listings\",\n    x = \"Instant Bookable\",\n    y = \"Number of Listings\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Drop any factor variable with only one level\nfactor_vars &lt;- sapply(airbnb_clean, is.factor)\nvalid_factors &lt;- names(which(sapply(airbnb_clean[, factor_vars], nlevels) &gt; 1))\nairbnb_clean &lt;- airbnb_clean %&gt;% select(all_of(c(\"number_of_reviews\", valid_factors, \n                                                  \"bathrooms\", \"bedrooms\", \"price\", \n                                                  \"review_scores_cleanliness\", \n                                                  \"review_scores_location\", \n                                                  \"review_scores_value\")))\n\n# Build model formula dynamically\npredictors &lt;- setdiff(names(airbnb_clean), \"number_of_reviews\")\nformula_text &lt;- paste(\"number_of_reviews ~\", paste(predictors, collapse = \" + \"))\nmodel_formula &lt;- as.formula(formula_text)\n\n# Fit the Poisson regression model\nairbnb_model &lt;- glm(model_formula, data = airbnb_clean, family = poisson(link = \"log\"))\n\n# Display model summary\nsummary(airbnb_model)\n\n\nCall:\nglm(formula = model_formula, family = poisson(link = \"log\"), \n    data = airbnb_clean)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                3.572e+00  1.600e-02 223.215  &lt; 2e-16 ***\nroom_typePrivate room     -1.453e-02  2.737e-03  -5.310 1.09e-07 ***\nroom_typeShared room      -2.519e-01  8.618e-03 -29.229  &lt; 2e-16 ***\ninstant_bookableTRUE       3.344e-01  2.889e-03 115.748  &lt; 2e-16 ***\nbathrooms                 -1.240e-01  3.747e-03 -33.091  &lt; 2e-16 ***\nbedrooms                   7.494e-02  1.988e-03  37.698  &lt; 2e-16 ***\nprice                     -1.436e-05  8.303e-06  -1.729   0.0838 .  \nreview_scores_cleanliness  1.132e-01  1.493e-03  75.821  &lt; 2e-16 ***\nreview_scores_location    -7.680e-02  1.607e-03 -47.796  &lt; 2e-16 ***\nreview_scores_value       -9.153e-02  1.798e-03 -50.902  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 961626  on 30159  degrees of freedom\nResidual deviance: 936528  on 30150  degrees of freedom\nAIC: 1058014\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "blog/project2/index.html#sub-header",
    "href": "blog/project2/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  }
]